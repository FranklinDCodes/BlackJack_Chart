

is it wrong when I am training to update the q values of a state based on looking up values in the table that may have been updated in this very example??
or should I save the q values that were there at the time of the decision in order to truely train in a batch??


should I make it so that agent cannot hit on 21 or naw?


first day of work:

    git ls-files | xargs wc -l
    15 .vscode/settings.json
    460 BlackJack.h
    443 BlackJackAgent.h
    wc: a.exe: No such file or directory
    14 driver.cpp
    16 test.cpp
    11 thoughts.txt
    959 total


    ~900 lines of code

